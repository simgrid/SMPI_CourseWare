<!DOCTYPE html>
<html lang="en-us">


<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Topic &#35;2 Module &middot; SMPI CourseWare
    
  </title>



  <!-- Semantic UI, JQuery, etc. -->
  <link rel="stylesheet" type="text/css" href="/SMPI_CourseWare//semantic/semantic.min.css">
  <link rel="stylesheet" type="text/css" href="/SMPI_CourseWare//public/basics.css">
  <link rel="stylesheet" type="text/css" href="/SMPI_CourseWare//public/syntax.css">

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script> -->
  <script src="/SMPI_CourseWare//jquery/jquery.min.js"></script>
  <script src="/SMPI_CourseWare//semantic/components/accordion.js"></script>
  <script src="/SMPI_CourseWare//semantic/components/popup.js"></script>
  <script src="/SMPI_CourseWare//semantic/components/transition.js"></script>
  <script src="/SMPI_CourseWare//semantic/components/tab.js"></script>

  <script language='javascript'>
               $(document).ready(function(){
               $('.ui.accordion').accordion();
               $('.popable')
                   .popup({
                     position: 'right center'
                   })
               ;
                 $('.menu .item').tab();

                 $('.paths.example .menu .item')
                     .tab({
                       context: '.paths.example'
                     })
                 ;

               });
  </script>


</head>



<body>

	

<div class="ui vertical left fixed menu basic  ">
  <br>
  <a href="http://simgrid.org" target="_blank">
  <img width=99 src="/SMPI_CourseWare/public/simgrid_logo.jpg" style="margin-left:5px">
  </a>
  <br>
  <br>




  <a class="ui item " href="/SMPI_CourseWare/">
    <h3 class="ui header">Home</h3>
  </a>


  
  
  
  
  
  <a class="ui popable item fluid " href="/SMPI_CourseWare//000_objectives/"
       data-content="Curricular information" data-variation="inverted">
      <h3 class="ui header fluid">About
      </h3>
  </a>
  
  
  
  
  
  
  
  <a class="ui popable item fluid " href="/SMPI_CourseWare//00_topic/"
       data-content="Getting started with SMPI" data-variation="inverted">
      <h3 class="ui header fluid">Topic &#35;0
      </h3>
  </a>
  
  
  
  
  
  
  
  <a class="ui popable item fluid " href="/SMPI_CourseWare//01_topic/"
       data-content="Basics of distributed memory programming" data-variation="inverted">
      <h3 class="ui header fluid">Topic &#35;1
      </h3>
  </a>
  
  
  
  
  
  
  
  <a class="ui popable item fluid  active" href="/SMPI_CourseWare//02_topic/"
       data-content="Communication on distributed platforms" data-variation="inverted">
      <h3 class="ui header fluid">Topic &#35;2
      </h3>
  </a>
  
  
  
  
  
  
  
  <a class="ui popable item fluid " href="/SMPI_CourseWare//03_topic/"
       data-content="Rigid distributed memory programs" data-variation="inverted">
      <h3 class="ui header fluid">Topic &#35;3
      </h3>
  </a>
  
  
  
  
  
  
  
  <a class="ui popable item fluid " href="/SMPI_CourseWare//04_topic/"
       data-content="Understanding performance" data-variation="inverted">
      <h3 class="ui header fluid">Topic &#35;4
      </h3>
  </a>
  
  
  
  
  
  
  
  <a class="ui popable item fluid " href="/SMPI_CourseWare//05_topic/"
       data-content="Flexible distributed memory programs" data-variation="inverted">
      <h3 class="ui header fluid">Topic &#35;5
      </h3>
  </a>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  <div class="ui divider" style="margin-top:0em"></div>

</div>



	<div class="contentpage container fluid">
	<div class="page">
  <br>
	
	<h1 class="ui header">Topic &#35;2 Module: Broadcast Communication</h1>
	
  <div class="ui divider"></div>

	<div class="ui pointing secondary menu">
  <a class="item active" data-tab="first">Introduction</a>
  <a class="item " data-tab="second">Activity #1</a>
  <a class="item" data-tab="third">Activity #2</a>
  <a class="item" data-tab="fourth">Activity #3</a>
  <a class="item" data-tab="fifth">Activity #4</a>
  <a class="item" data-tab="sixth">Activity #5</a>
  <a class="item" data-tab="seventh">Conclusion</a>
</div>

<div class="ui tab segment active" data-tab="first">
  <div class="ui container raised segment fluid">

  <h3 class="ui header">Overview</h3>

  <p class="ui">
    <b>Objective: </b> To implement several versions of a broadcast collective (i.e., a one-to-many communication),
    and to compare them in various platform setups.
  </p>

</div>

<div class="ui container segment raised fluid">
  <h3 class="ui header">Roadmap</h3>
  <p class="ui">This module consists of <b>4 activities, each described in its own tab above, which should be done in
    sequence:</b>
  </p>
  <ul class="ui list">
    <li class="item"><b>Activity #1:</b> Realize a naive and a ring-based implementation.
    </li>
    <li class="item"><b>Activity #2:</b> Enhance the ring-based implementation with pipelining.
    </li>
    <li class="item"><b>Activity #3:</b> Enhance the ring-based implementation further using asynchronous communication.
    </li>
    <li class="item"><b>Activity #4:</b> Implement a binary-tree-based implementation (with pipelining and asynchronous communication).
    </li>
  </ul>
  <p class="ui">In all activities above you will compare your implementation with <code>MPI_Bcast</code>, for different platform configurations.</p>

</div>


<div class="ui container segment raised fluid">
  <h3 class="ui header">Skeleton Program</h3>
  <p class="ui">
    Throughout this module, you will be adding code to a "skeleton program" (download <a href="bcast_skeleton.c">bcast_skeleton.c</a>):

  <div class="ui accordion fluid">
    <div class=" title">
      <i class="dropdown icon"></i>
      See the source code of bcast_skeleton.c ...
    </div>
    <div class=" content">
      <div class="ui raised container segment ">

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="cp">#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;unistd.h&gt;
#include &lt;mpi.h&gt;
#include &lt;string.h&gt;
</span>
<span class="c1">// See for the (bad) default random number generator</span>
<span class="cp">#define RAND_SEED 842270
</span>
<span class="c1">// Number of bytes to broadcast</span>

<span class="cp">#define NUM_BYTES 100000000
</span>
<span class="c1">///////////////////////////////////////////////////////</span>
<span class="c1">//// program_abort() and print_usage() functions //////</span>
<span class="c1">///////////////////////////////////////////////////////</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">program_abort</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">exec_name</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">message</span><span class="p">);</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_usage</span><span class="p">();</span>

<span class="c1">// Abort, printing the usage information only if the</span>
<span class="c1">// first argument is non-NULL (and hopefully set to argv[0]), and</span>
<span class="c1">// printing the second argument regardless.</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">program_abort</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">exec_name</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">message</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">my_rank</span><span class="p">;</span>
  <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="o">&amp;</span><span class="n">my_rank</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">my_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">message</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"%s"</span><span class="p">,</span><span class="n">message</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">exec_name</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">print_usage</span><span class="p">(</span><span class="n">exec_name</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="n">MPI_Abort</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
  <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// Print the usage information</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">print_usage</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">exec_name</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">my_rank</span><span class="p">;</span>
  <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="o">&amp;</span><span class="n">my_rank</span><span class="p">);</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">my_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"Usage: smpirun --cfg=smpi/bcast:mpich -np &lt;num processes&gt;</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"              -platform &lt;XML platform file&gt; -hostfile &lt;host file&gt;</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"              %s &lt;bcast implementation name&gt; [-c &lt;chunk size&gt;]</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">exec_name</span><span class="p">);</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"MPIRUN arguments:</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"</span><span class="se">\t</span><span class="s">&lt;num processes&gt;: number of MPI processes</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"</span><span class="se">\t</span><span class="s">&lt;XML platform file&gt;: a Simgrid platform description file</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"</span><span class="se">\t</span><span class="s">&lt;host file&gt;: MPI host file with host names from the platform file</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"PROGRAM arguments:</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"</span><span class="se">\t</span><span class="s">&lt;bcast implementation name&gt;: the name of the broadcast implementaion (e.g., naive_bcast)</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"</span><span class="se">\t</span><span class="s">[-c &lt;chunk size&gt;]: chunk size in bytes for message splitting (optional)</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">return</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">///////////////////////////</span>
<span class="c1">////// Main function //////</span>
<span class="c1">///////////////////////////</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
  <span class="kt">int</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">NUM_BYTES</span><span class="p">;</span>
  <span class="kt">char</span> <span class="o">*</span><span class="n">bcast_implementation_name</span><span class="p">;</span>

  <span class="c1">// Parse command-line arguments (not using getopt because not thread-safe</span>
  <span class="c1">// and annoying anyway). The code below ignores extraneous command-line</span>
  <span class="c1">// arguments, which is lame, but we're not in the business of developing</span>
  <span class="c1">// a cool thread-safe command-line argument parser.</span>

  <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>

  <span class="c1">// Bcast implementation name</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">program_abort</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s">"Missing &lt;bcast implementation name&gt; argument</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="n">bcast_implementation_name</span> <span class="o">=</span> <span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
  <span class="p">}</span>

  <span class="c1">// Check that the implementation name is valid</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">strcmp</span><span class="p">(</span><span class="n">bcast_implementation_name</span><span class="p">,</span> <span class="s">"naive_bcast"</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
      <span class="n">strcmp</span><span class="p">(</span><span class="n">bcast_implementation_name</span><span class="p">,</span> <span class="s">"default_bcast"</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
      <span class="n">strcmp</span><span class="p">(</span><span class="n">bcast_implementation_name</span><span class="p">,</span> <span class="s">"ring_bcast"</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
      <span class="n">strcmp</span><span class="p">(</span><span class="n">bcast_implementation_name</span><span class="p">,</span> <span class="s">"pipelined_ring_bcast"</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
      <span class="n">strcmp</span><span class="p">(</span><span class="n">bcast_implementation_name</span><span class="p">,</span> <span class="s">"asynchronous_pipelined_ring_bcast"</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
      <span class="n">strcmp</span><span class="p">(</span><span class="n">bcast_implementation_name</span><span class="p">,</span> <span class="s">"asynchronous_pipelined_bintree_bcast"</span><span class="p">))</span> <span class="p">{</span>
    <span class="kt">char</span> <span class="n">message</span><span class="p">[</span><span class="mi">256</span><span class="p">];</span>
    <span class="n">sprintf</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="s">"Unknown bcast implementation name '%s'</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">bcast_implementation_name</span><span class="p">);</span>
    <span class="n">program_abort</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span><span class="n">message</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="c1">// Chunk size optional argument</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">argc</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">strcmp</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="s">"-c"</span><span class="p">))</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">((</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span> <span class="o">&gt;=</span> <span class="n">argc</span><span class="p">)</span> <span class="o">||</span> <span class="p">(</span><span class="n">sscanf</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span><span class="s">"%d"</span><span class="p">,</span><span class="o">&amp;</span><span class="n">chunk_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">program_abort</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s">"Invalid &lt;chunk size&gt; argument</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
  
  <span class="c1">// Determine rank and number of processes</span>
  <span class="kt">int</span> <span class="n">num_procs</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">rank</span><span class="p">;</span>
  <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rank</span><span class="p">);</span>
  <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">num_procs</span><span class="p">);</span>

  <span class="c1">// Allocate buffer</span>
  <span class="kt">int</span> <span class="n">checksum</span><span class="p">;</span>
  <span class="kt">char</span> <span class="o">*</span><span class="n">buffer</span><span class="p">;</span>
  
  <span class="k">if</span> <span class="p">((</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">char</span><span class="p">)</span> <span class="o">*</span> <span class="n">NUM_BYTES</span><span class="p">))</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">program_abort</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s">"Out of memory!"</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="c1">// On rank 0 fill the buffer with random data </span>
  <span class="k">if</span> <span class="p">(</span><span class="mi">0</span> <span class="o">==</span> <span class="n">rank</span><span class="p">)</span> <span class="p">{</span> 
    <span class="n">checksum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">srandom</span><span class="p">(</span><span class="n">RAND_SEED</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_BYTES</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">buffer</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kt">char</span><span class="p">)</span> <span class="p">(</span><span class="n">random</span><span class="p">()</span> <span class="o">%</span> <span class="mi">256</span><span class="p">);</span> 
      <span class="n">checksum</span> <span class="o">+=</span> <span class="n">buffer</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="c1">// Start the timer</span>
  <span class="kt">double</span> <span class="n">start_time</span><span class="p">;</span>
  <span class="n">MPI_Barrier</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>  
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">MPI_Wtime</span><span class="p">();</span>
  <span class="p">}</span>

  <span class="c1">/////////////////////////////////////////////////////////////////////////////</span>
  <span class="c1">//////////////////////////// TO IMPLEMENT: BEGIN ////////////////////////////</span>
  <span class="c1">/////////////////////////////////////////////////////////////////////////////</span>

  <span class="c1">// char *bcast_implementation_name:   the bcast implementation name (argument #1)</span>
  <span class="c1">// int chunk_size:                    the chunk size (optional argument #2)</span>
  <span class="c1">// int NUM_BYTES:                     the number of bytes to broadcast</span>
  <span class="c1">// char *buffer:                      the buffer to broadcast</span>

  <span class="c1">// Process rank 0 should be  the source of the broadcast</span>

<span class="cp">#include "bcast_solution.c"
</span>
  <span class="c1">/////////////////////////////////////////////////////////////////////////////</span>
  <span class="c1">///////////////////////////// TO IMPLEMENT: END /////////////////////////////</span>
  <span class="c1">/////////////////////////////////////////////////////////////////////////////</span>
 
  <span class="c1">// All processes send checksums back to the root, which checks for consistency</span>
  <span class="kt">char</span> <span class="n">all_ok</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="mi">0</span> <span class="o">==</span> <span class="n">rank</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">num_procs</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">int</span> <span class="n">received_checksum</span><span class="p">;</span>
      <span class="n">MPI_Recv</span><span class="p">(</span><span class="o">&amp;</span><span class="n">received_checksum</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">MPI_ANY_SOURCE</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="n">MPI_STATUS_IGNORE</span><span class="p">);</span>
      <span class="c1">// Print a single message in case of a mismatch, but continue</span>
      <span class="c1">// receiving other checksums to ensure that all processes</span>
      <span class="c1">// reach the MPI_Finalize()</span>
      <span class="k">if</span> <span class="p">((</span><span class="n">all_ok</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">checksum</span> <span class="o">!=</span> <span class="n">received_checksum</span><span class="p">))</span> <span class="p">{</span>
	    <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="s">"</span><span class="se">\t</span><span class="s">** Non-matching checksum! **</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
	    <span class="n">all_ok</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	    <span class="k">break</span><span class="p">;</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">checksum</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_BYTES</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">checksum</span> <span class="o">+=</span> <span class="n">buffer</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="n">MPI_Send</span><span class="p">(</span><span class="o">&amp;</span><span class="n">checksum</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="c1">// Print out bcast implementation name and wall-clock time, only if the bcast was successful</span>
  <span class="n">MPI_Barrier</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">((</span><span class="mi">0</span> <span class="o">==</span> <span class="n">rank</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">all_ok</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">fprintf</span><span class="p">(</span><span class="n">stdout</span><span class="p">,</span><span class="s">"implementation: %s | chunksize: %d |  time: %.3lf seconds</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span>
		    <span class="n">bcast_implementation_name</span><span class="p">,</span> 
		    <span class="n">chunk_size</span><span class="p">,</span>
		    <span class="n">MPI_Wtime</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="c1">// Clean-up</span>
  <span class="n">free</span><span class="p">(</span><span class="n">buffer</span><span class="p">);</span>
  <span class="n">MPI_Finalize</span><span class="p">();</span>

  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

      </div>
    </div>
  </div>
  </p>

  <p class="ui">
    This program performs a broadcast of a 10<sup>8</sup> bytes (~ 95 MiB) and takes the following command-line arguments:
  <div class="ui list bulleted">
    <div class="ui item">required argument #1: a string that is the name of the broadcast implementation (see each activity)</div>
    <div class="ui item">optional argument #2 (<i> [-c chunksize]</i>): an integer that specifies a "chunk size" (see activity #2)</div>
  </div>
  </p>
  <p class="ui">
    <b>You should not modify <code>bcast_skeleton.c</code> outside of the "<code>TO IMPLEMENT: BEGIN</code>" and
      "<code>TO IMPLEMENT: END</code>" comments.</b>  See comments therein for all needed information to implement
    your code. Note that you can opt to write your code directly in between these two comments,
    or <code>#include</code> it, up to you.  Regardless, the goal is to have a single program,
    <code>bcast_skeleton</code>, that is executed with different command-line arguments for
    all activities in this module.
  </p>

  <p class="ui">
    The skeleton program does the following for you:  parsing of command-line arguments, creation of broadcast data,
    checking that the broadcast operation was done correctly, timing of the broadcast operation.
  </p>

</div>


<div class="ui container segment raised fluid">
  <h3 class="ui header">What to turn in</h3>
  <p class="ui">You should turn in a single archive (or a github repo) with:
  <div class="ui bulleted list">
    <div class="item">All source code</div>
    <div class="item">XML platform files and hostfiles (see details in the activities)</div>
    <div class="item">A Makefile that compiles all executables (and has a 'clean' target!)</div>
    <div class="item">A README file with answers to the questions asked in the activities</div>
  </div>
  </p>
</div>
</div>

<div class="ui tab segment " data-tab="second">
  <div class="ui container segment raised">In this activity we develop <b>two simple broadcast implementations</b>,
  in the first two steps below.
You can verify correctness of your broadcast implementations using any SimGrid platform file
and corresponding MPI hostfile. In the third step below we evaluate performance for a particular
platform file.
</div>

<div class="ui top attached tabular menu">
  <a class="item active" data-tab="naive_broadcast">Step #1: Naive Broadcast</a>
  <a class="item" data-tab="ring_broadcast">Step #2: Ring Broadcast</a>
  <a class="item" data-tab="naive_ring_broadcast_evaluation">Step #3: Evaluation</a>
</div>


<div class="ui bottom attached tab segment active" data-tab="naive_broadcast">

  


<p class="ui">
  Augment the <code>bcast_skeleton.c</code> program with a broadcast implementation named
  <b><code>naive_bcast</code></b> (first command-line argument). This implementation simply
  has the root of the broadcast (process with rank 0) perform a single
  point-to-point <code>MPI_Send</code> of the whole buffer to each other process (which performs an <code>MPI_Recv</code>).
  In other words, the processes are logically organized as
  a <i>mono-directional star</i> with process of rank 0 at the center.
  The second command-line argument (chunk size) is ignored.
</p>




</div>


<div class="ui bottom attached tab segment" data-tab="ring_broadcast">

  <p class="ui">
  Augment the <code>bcast_skeleton.c</code> program with a broadcast implementation named
  <b><code>ring_bcast</code></b> (first command-line argument). In this implementation,
  the process of rank 0 (the root of the broadcast) does a single <code>MPI_Send</code> of the
  whole buffer to the process of rank 1, the process of rank 1 then does a single <code>MPI_Send</code>
  to the process of rank 2, etc. In other words, the processes are logically organized as
  a <i>mono-directional ring</i>.  The last process should not send the buffer back to the root process (the root process
  does not call <code>MPI_Recv</code>).
  The second command-line argument (chunk size) is ignored.
</p>


</div>


<div class="ui bottom attached tab segment" data-tab="naive_ring_broadcast_evaluation">

  <p class="ui">
  Before you proceed with the evaluation of your two implementations, augment <code>bcast_skeleton.c</code>
  with a third broadcast implementation named <b>default_bcast</b> that simply has all processes
  call <code>MPI_Bcast</code>. This is the MPI implementation of the broadcast collective communication,
  which in this module we implement "by hand" using point-to-point communications.
</p>

<p class="ui">
  We compare the performance of the three implementations on a <b>50-processor physical ring</b>. Although some
  supercomputers
  in the old days were designed with a ring network topology, this is no longer the case. The main drawback of a
  physical
  ring is that it has very large diameter (i.e., there can be ~n/2 hops between two processors in an n-processor ring).
  The main advantages
  is that the degree is low (2), which implies low cost, and that routing is straightforward. For now we assume a simple
  physical ring
  so as to better understand broadcast performance.
</p>

<p class="ui">
  It is pretty simple to generate a Simgrid platform file for a ring and the corresponding hostfile.
  You can download a Python script <a href="./generate_xml_ring_and_hostfile.py">(generate_xml_ring_and_hostfile.py)</a>
  that generates
  these two files given a number of processors passed as a command-line argument. Just in case,
  here are the files generated using this script for 50 processors: <a href="ring_50.xml">ring_50.xml</a> and
  <a href="hostfile_50.txt">hostfile_50.txt</a>.
</p>

<p class="ui">
  The way to invoke the program is as follows:
</p>

<div class="ui container raised segment">
  
<figure class="highlight"><pre><code class="language-text" data-lang="text">  smpirun --cfg=smpi/bcast:mpich -np 50 
  -platform ring_50.xml -hostfile hostfile_50.txt
  ./bcast_skeleton &lt;implementation name&gt;
  </code></pre></figure>

</div>

<p class="ui">
  The <code>--cfg=smpi/bcase:mpich</code> flag above specifies that we simulate <code>MPI_Bcast</code> (for the
  default_bcast implementation) as implemented
  in MPICH. Other options are possible, but it's okay to stick with this implementation
</p>

<p class="ui">
  Answer the following questions:
<div class="ui list bulleted">
  <div class="ui item">What are the (simulated) wall-clock time of the three implementations on the 50-processor
    ring?
  </div>
  <div class="ui item">How far are your "by hand" implementations from the default
    broadcast?
  </div>
  <div class="ui item">You may observe that ring_bcast does not improve a lot over naive_bcast, which should
    be surprising to you. After all, naive_bcast sends long-haul messages while ring_bcast doesn't
    What do you think the reason
    is? To answer this question, you can instrument your code and run it on smaller rings to see when events happen and
    try to understand what's going on. Given that we're using simulation, you should take advantage of it and
    experiment with all kinds of platform configurations to gain understanding of the performance behavior. For
    instance,
    you can modify link latencies and bandwidths.
    The <code>MPI_Wtime</code> function is convenient to determine the current (simulated) date. This
    function returns the date as a double precision number (and is in fact already used in <code>bcast_skeleton.c</code>).
  </div>
</div>
</p>

<p class="ui">
  <i>Warning:</i> SMPI implements sophisticated simulation models that capture many real-world effects (network
  protocol idiosyncrasies, MPI implementation idiosyncrasies).
  These effects will be seen in your experiments, just as they would on a real-world platform,
  and they tend to make performance behavior more difficult to understand. For instance, if you modify
  the buffer size, you will see non-linear effects on performance (e.g., broadcasting a buffer twice as large
  will not require exactly twice as much time, broadcasting a buffer 1 byte larger may increase broadcast time significantly).
</p>


</div>

</div>

<div class="ui tab segment" data-tab="third">
  <div class="ui container raised segment">In this activity we enhance the ring-based broadcast so that it 
  uses <b>pipelining</b> to better
utilize the network links.
</div>

<div class="ui top attached tabular menu">
  <a class="item active" data-tab="pipelined_broadcast">Step #1: Pipelined Broadcast</a>
  <a class="item" data-tab="pipelined_broadcast_evaluation">Step #2: Evaluation</a>
</div>


<div class="ui bottom attached tab segment active" data-tab="pipelined_broadcast">

  <p class="ui">
  One of the reasons why the default MPI broadcast is fast is that it does <i>pipelining</i>
  of communication. Augment the <code>bcast_skeleton.c</code> program with a new broadcast implementation
  called <b>pipelined_ring_bcast</b>.
</p>

<p class="ui">This implementation uses the chunk size passed as the second command-line argument
  to split the broadcast data buffer into multiple chunks. These chunks are communicated in sequence along
  the ring. In the first step, process #0 sends chunk #0 to process #1; in the second step, process #1 sends chunk #0 to
  process #2
  and process #0 sends chunk #1 to process #1; and so on. As a result of this pipelining, provided the chunk size is
  small
  enough, all network links can be utilized simultaneously.  As in the previous activity,
  use <code>MPI_Send</code> and <code>MPI_Recv</code> for
  communications.
</p>

<p class="ui">
  Note that the chunk size may not divide the message size, in which case the last chunk will be smaller.
</p>




</div>


<div class="ui bottom attached tab segment" data-tab="pipelined_broadcast_evaluation">

  <p class="ui">
  Run pipelined_ring_bcast using chunk sizes of
  100,000 bytes,
  500,000 bytes,
  1,000,000 bytes,
  5,000,000 bytes,
  10,000,000 bytes,
  50,000,000 bytes, and
  100,000,000 (i.e., 1 chunk),
  on a 20-processor ring, a 35-processor ring, and a 50-processor ring. Report the
  simulated execution times. Remember that the <a href="./generate_xml_ring_and_hostfile.py">generate_xml_ring_and_hostfile.py
  </a>Python script can be used to generate platform files and hostfiles.
</p>

<p class="ui">
  Answer the following questions:
<div class="ui list bulleted">
  <div class="ui item">
    What is the best chunk size for each of the three platforms?
  </div>
  <div class="ui item">Does this chunk size depend on the platform size?</div>
  <div class="ui item">You should observe that the wall-clock time is minimized for a chunk size that is neither the
    smallest nor the largest.
    Discuss why you think that is.
  </div>
  <div class="ui item">For a 100-processor ring, with the best chunk size determined above,
    by how much does pipelining help when compared to using a single chunk?
  </div>
  <div class="ui item"> How does the performance compare to that of the default MPI broadcast for that platform as seen in
    the previous question?
  </div>
  <div class="ui item">What do you conclude about the use of pipelined communications for the purpose
    of a ring-based broadcast on a ring-shaped physical platform?</div>
</div>
</p>

<p class="ui">
  <b>Warning: </b>  You will likely see some "odd" behavior. For instance, as the chunk size increases you may
  see the broadcast time decrease, increase, and then decrease again.  Or, if ou try very small chunk sizes,
  smaller than the ones above, you could see that for some particular chunk sizes the broadcast time is
  pretty low, if perhaps not the lowest across the board. These effects are because our simulated MPI captures
  many effects of network protocols and  MPI implementations. These effects take us away from simple analytical models of communication
  performance, in which there is no weirdness, but that simply don't match the reality of communication
  behavior in real-world parallel platforms. As a result, experimental results are often confusing and it is difficult
  to draw valid conclusions without looking at a lot of results.
</p>




</div>

</div>

<div class="ui tab segment" data-tab="fourth">
  <div class="ui container segment raised">In this activity we enhance the ring-based pipelined broadcast 
  so that it uses <b>asynchronous communication</b>,
to use network links even better.
</div>

<div class="ui top attached tabular menu">
  <a class="item active" data-tab="asynchronous_broadcast">Step #1: Asynchronous Broadcast</a>
  <a class="item" data-tab="asynchronous_broadcast_evaluation">Step #2: Evaluation</a>
</div>


<div class="ui bottom attached tab segment active" data-tab="asynchronous_broadcast">

  <p class="ui">
  In our current implementation of the broadcast, we use <code>MPI_Send</code>  and
  <code>MPI_Recv</code>. One drawback of this approach is that a process can only
  send or receive data at a time. However, it would make sense for a process
  to send chunk #i to its successor while receiving chunk #i+1 from its predecessor. Intuitively,
  it should make it possible to hide network overheads better and
  perhaps to achieve higher data transfer rates.
  Augment the <code>bcast_skeleton.c</code> program with a new broadcast implementation
  called <b>asynchronous_pipelined_ring_bcast</b>. This implementation builds on the
  pipelined_ring_bcast implementation but uses the following MPI asynchronous communication primitives:
  <code>MPI_Isend</code> and <code>MPI_Wait</code>.
</p>

<p class="ui">
  Although in principle simple, adding asynchronous communication is often not done correctly
  by beginner MPI programmers. Here are common mistakes to avoid when implementing asynchronous_pipelined_ring_bcast:
  <div class="ui list bulleted">
  <div class="ui item">Each call to <code>MPI_Isend</code> must have a matching <code>MPI_Wait</code> call.</div>

  <div class="ui item">
                        The call to <code>MPI_Isend</code> for chunk #i must be issued only after the
                        call to <code>MPI_Recv</code> for chunk #i (since one must wait to have received a chunk to forward it
                       to one's successor).</div>

  <div class="ui item">Having a call to <code>MPI_Isend</code> immediately followed by its matching 
                       <code>MPI_Wait</code> call is equivalent to a single <code>MPI_Send</code> call, i.e., it
                       does not implement asynchronous communication! </div>

  <div class="ui item">Given the above, and given that the intent is that one asynchronously sends chunk #i to one's
                       successor while receiving chunk #i+1 from one's predecessor, you should be able to determine
                       the sequence of communication operations at each process. Note that in all the above
                        the root process is a special case because it only sends and never receives. And so
                        is the last process since it only receives and never sends.</div>

</div>
</p>



</div>


<div class="ui bottom attached tab segment" data-tab="asynchronous_broadcast_evaluation">

  <p class="ui">
  Run asynchronous_pipelined_ring_bcast using chunk sizes of 100,000 bytes,
  500,000 bytes,
  1,000,000 bytes,
  5,000,000 bytes,
  10,000,000 bytes,
  50,000,000 bytes, and
  100,000,000 (i.e., 1 chunk)
  on a 50-processor ring. Report the
  simulated execution times. Remember that the <a href="./generate_xml_ring_and_hostfile.py">generate_xml_ring_and_hostfile.py
  </a>Python script can be used to generate platform files and hostfiles.
</p>

<p class="ui">
  Answer the following questions:
<div class="ui list bulleted">
  <div class="ui item">
    Is the best chunk size the same as that for pipelined_ring_bcast?
    </div>
  <div class="ui item">
  When using the best chunk size for each of the two
    implementations (which may be the same if the answer to the previous question is "no"),
    does the use of <code>MPI_Isend</code> help? By how much?</div>

  <div class="ui item">Is asynchronous_pipelined_ring_bcast faster than default_bcast or slower? By how much? </div>
  <div class="ui item">What do you conclude about the use of asynchronous communications for the purpose
        of a ring-based broadcast on a ring-shaped physical platform?</div>

</div>
</p>




</div>

</div>

<div class="ui tab segment" data-tab="fifth">
  <div class="ui container segment raised">In this activity we shift gear completely and abandon the idea of a ring-based broadcast. Instead
we organize the processes as a <b>logical binary tree</b>. The root of the broadcast is also at
the root of the tree. In practice, many broadcast algorithms are tree-based.
  And several real-world systems comprise tree-shaped network topologies.
  We will
evaluate this new broadcast implementation on both a ring-shaped physical platform
and a tree-shaped physical platform.</div>

<div class="ui top attached tabular menu">
  <a class="item active" data-tab="bintree_broadcast">Step #1: Binary Tree Broadcast</a>
  <a class="item" data-tab="bintree_broadcast_evaluation">Step #2: Evaluation</a>
</div>


<div class="ui bottom attached tab segment active" data-tab="bintree_broadcast">

  <p class="ui">
  Augment your code with an implementation called
  <b>asynchronous_pipelined_bintree_bcast</b> that
  implements the broadcast along a binary tree, splitting the message into chunks for
  <b>pipelining</b> and using <b>asynchronous communication</b>. The root of the broadcast,
  process of rank #0, is also the root of the binary tree. Note that the binary tree may
  not be complete if the number of processors is not of the form 2<sup>n</sup> − 1.
  </p>


<p class="ui">
  Use a <b>level order traversal numbering</b> of the nodes in your tree:
  rank 0 has rank 1 as its left child and rank 2 as its right child,
  rank 1 has rank 3 as its left child and rank 4 as its right child,
  etc. The reason why we impose this order is that this is also the order used to number
  processes in the platform description file used for experimental evaluations (see Step #2). The idea
  is that the logical binary tree of processes is identical to the physical binary tree of processors
  (process #i runs on processor #i).
</p>





</div>


<div class="ui bottom attached tab segment" data-tab="bintree_broadcast_evaluation">

  <p class="ui">
  Report the (simulated) wall-clock time of
  default_bcast,
  asynchronous_pipelined_ring_bcast, and
  asynchronous_pipelined_bintree_bcast, on
  a 50-processor ring platform and a 50-processor binary tree platform.
  For asynchronous_pipelined_ring_bcast and
  asynchronous_pipelined_bintree_bcast use the "best chunksize" you determined
  in Activity #2 for the 50-processor ring platform.
  </p>

<p class="ui">
  You are provided with a Python script
  (<a href="./generate_xml_bintree_and_hostfile.py">generate_xml_bintree_and_hostfile.py</a>)
  that generates the XML file (and hostfile) for the binary tree platform. As for
  the <a href="./generate_xml_bintree_and_hostfile.py">generate_xml_bintree_and_hostfile.py</a> script,
  the number of processors is passed as a command-line
  argument. Just in case,
  here are the files generated using this script for 50 processors: <a href="bintree_50.xml">bintree_50.xml</a> and
  <a href="hostfile_50.txt">hostfile_50.txt</a>.
</p>

<p class="ui">
  Answer the following questions:
<div class="ui list bulleted">
  <div class="ui item">
    On the 50-processor ring platform, how does asynchronous_pipelined_bintree_bcast compare
    to asynchronous_pipelined_ring_bcast?  Is it unexpected?
    </div>
  <div class="ui item">
    On the 50-processor binary tree platform, how do the three implementations compare?
    Does it seem worth it to implement your own binary tree broadcast on a binary tree physical
    platform?
  </div>

</div>
</p>




</div>

</div>

<div class="ui tab segment" data-tab="sixth">
  <div class="ui container segment raised">
  Many high-performance computing platforms these days are built using crossbar switches. Although
  the topology of these switches can itself be a ring, a tree, or anything else, in many cases
  sizeable platforms are deployed with only a small number of swicthes. In this activity
  we evaluate our broadcast  implementation on two commonplace cluster platform configurations.

</div>

<div class="ui top attached tabular menu">
  <a class="item active" data-tab="cluster_evaluation">Step #1: On clusters?</a>
</div>


<div class="ui bottom attached tab segment active" data-tab="cluster_evaluation">

  <p class="ui">
  Report the (simulated) wall-clock time of
  default_bcast,
  asynchronous_pipelined_ring_bcast, and
  asynchronous_pipelined_bintree_bcast, on the following two
  platforms:

  <div class="ui list bulleted">

  <div class="ui item">A 64-processor cluster based on a single crossbar switch:
  <a href="./cluster_crossbar_64.xml">cluster_crossbar_64.xml</a>.
  </div>

  <div class="ui item">A 64-processor cluster with a shared backbone:
    <a href="./cluster_backbone_64.xml">cluster_backbone_64.xml</a>.
  </div>
</div>

  You can use this hostfile: <a href="./hostfile_64.txt">hostfile_64.txt</a>.

  For asynchronous_pipelined_ring_bcast and
  asynchronous_pipelined_bintree_bcast use the "best chunksize" you determined
  in Activity #2 for the 50-processor ring platform.
  </p>


<p class="ui">
  Answer the following question:
<div class="ui list bulleted">
  <div class="ui item">
    Overall, does it seem like it's a good idea to use the default MPI broadcast on these more standard platforms,
    or should one implement one's own?
    Note that the default broadcast has to pick a particular chunk size while we are "cheating" by picking
    an empirically good chunk size for our implementations!
  </div>

</div>
</p>




</div>


</div>

<div class="ui tab segment" data-tab="seventh">
  <div class="ui container raised segment fluid">

  <h3 class="ui header">What have we learned?</h3>

  <p class="ui">
    Congratulations for completing this module. At this point you have learned how
    to use point-to-point MPI communication, both synchronous and asynchronous;
    you have learned about the most common collective communication primitive,
    the broadcast; you have observed first hand the connection between communication
    patterns and the underlying network topology; you have learned how pipelining
    and asynchronous communication can boost communication performance.
  </p>

  <p class="ui">
    As part of our experimental evaluations, you have observed the overall good
    performance of the default <code>MPI_Bcast</code> implementation. It would be very
    enlightening to look at open source MPI implementations and inspect the code
    (although it is intricate) to understand how the broadcast is implemented. In general,
    few developers implement broadcast by hand. However, in some cases (see upcoming modules),
    it can be useful to use a by-hand implementation of a broadcast to achieve better
    performance (in general, better overlap of communication and computation).
  </p>



</div>

<div class="ui container raised segment fluid">

  <h3 class="ui header">What was difficult?</h3>

  <p class="ui">
    At the end of your report, write a brief "essay" explaining what you found
    surprising/challenging, and what solutions you used to solve
    your difficulties. Are you happy with your implementations? Do you
    see room for improvement?
  </p>


</div>


<div class="ui container raised segment fluid">

<div class="ui horizontal divider">
  The End
</div>

</div>
</div>



  <br>

</div>
	</div>



</body>
</html>
